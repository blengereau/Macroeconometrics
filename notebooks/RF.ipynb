{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6a138935-1a32-411e-9de1-0f5bf8d74029",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Librairies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import contextlib\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/onyxia/work/Macroeconometrics')\n",
    "from src.preprocessing import apply_transformation\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2729921f-b723-4ea3-8000-49bdb290f69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main data\n",
    "fred_md = pd.read_csv(\"/home/onyxia/work/Macroeconometrics/data/fred_md_2024_12.csv\")\n",
    "#Metadata\n",
    "fred_info = pd.read_csv(\"/home/onyxia/work/Macroeconometrics/data/FRED_MD_updated_appendix.csv\", encoding=\"latin1\")\n",
    "#Recession variable\n",
    "us_rec = pd.read_csv(\"/home/onyxia/work/Macroeconometrics/data/USREC.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5aa3d16c-7430-48a6-a18b-4e4a8e462a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Indexing the dataset with dates\n",
    "fred_md_short = (\n",
    "    fred_md.iloc[1:]\n",
    "    .assign(sasdate=pd.to_datetime(fred_md.iloc[1:].sasdate, format=\"%m/%d/%Y\"))\n",
    "    .set_index(\"sasdate\")\n",
    ")\n",
    "\n",
    "#Transformation of series based on metadata\n",
    "for _, row in fred_info.iterrows():\n",
    "    series_name = row['fred']\n",
    "    transformation_code = row['tcode']\n",
    "\n",
    "    with contextlib.suppress(Exception):\n",
    "        fred_md_short[series_name] = apply_transformation(fred_md_short[series_name], transformation_code)\n",
    "\n",
    "#Filtering data by date\n",
    "start_date = \"1960\"\n",
    "end_date = \"2024\"\n",
    "fred_md_short = fred_md_short[\n",
    "    (fred_md_short.index >= start_date) & (fred_md_short.index <= end_date)\n",
    "].dropna(axis=1)\n",
    "\n",
    "#Addition of the variable of interest (American recession)\n",
    "us_rec = us_rec.assign(\n",
    "    observation_date=pd.to_datetime(us_rec.observation_date)\n",
    ").set_index(\"observation_date\")\n",
    "us_rec = us_rec.loc[fred_md_short.index,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "29ac2d87-9add-4935-9b63-d9e5272c94b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function Random Forest\n",
    "def run_random_forest(X, y, param_grid=None, test_size=0.2, random_state=667, cv_folds=5):\n",
    "    \"\"\"\n",
    "    Perform Random Forest classification with hyperparameter tuning using GridSearchCV.\n",
    "\n",
    "    Parameters:\n",
    "    X : Covariates (features) for training the model.\n",
    "    y : Target variable for classification.\n",
    "    param_grid : Dictionary of hyperparameters to tune.\n",
    "    test_size : Fraction of data to use for testing (default is 0.2).\n",
    "    random_state : Random seed for reproducibility (default is 42).\n",
    "    cv_folds : Number of folds for cross-validation (default is 5).\n",
    "    \n",
    "    Returns:\n",
    "    dict: Contains 'best_params', 'classification_report', 'accuracy', and 'feature_importance'.\n",
    "    \"\"\"\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Split the data into train and test sets (time-series split without shuffling)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=test_size, shuffle=False, random_state=random_state)\n",
    "    \n",
    "    # Instantiate Random Forest model\n",
    "    rf = RandomForestClassifier(random_state=random_state)\n",
    "    \n",
    "    # Hyperparameter tuning using GridSearchCV\n",
    "    if param_grid is None:\n",
    "        param_grid = {\n",
    "        'n_estimators': [1000],  # Nombre d'arbres dans la forêt\n",
    "        'max_depth': [None, 10, 30],   # Profondeur maximale des arbres\n",
    "        'min_samples_split': [2, 5, 10],   # Nombre minimal d'échantillons pour diviser un noeud\n",
    "        'min_samples_leaf': [1, 4],     # Nombre minimal d'échantillons par feuille\n",
    "        }\n",
    "    \n",
    "    grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=cv_folds, n_jobs=-1, verbose=2)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    #Best model\n",
    "    best_rf = grid_search.best_estimator_\n",
    "\n",
    "    #Predictions on the test set\n",
    "    y_pred = best_rf.predict(X_test)\n",
    "\n",
    "    #Evaluation\n",
    "    classification_rep = classification_report(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    #Feature importance\n",
    "    importances = best_rf.feature_importances_\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'Feature': X.columns if isinstance(X, pd.DataFrame) else [f'PC{i+1}' for i in range(X.shape[1])],\n",
    "        'Importance': importances\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "    # Return results\n",
    "    return {\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'classification_report': classification_rep,\n",
    "        'accuracy': accuracy,\n",
    "        'feature_importance': feature_importance_df\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf44473-f241-473a-b35a-a3c1da05a3ca",
   "metadata": {},
   "source": [
    "# RF naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dacfaf60-1461-44b5-953c-0e86a3b0695b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Best Hyperparameters : {'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 1000}\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       152\n",
      "           1       0.50      0.50      0.50         2\n",
      "\n",
      "    accuracy                           0.99       154\n",
      "   macro avg       0.75      0.75      0.75       154\n",
      "weighted avg       0.99      0.99      0.99       154\n",
      "\n",
      "\n",
      "Accuracy: 0.987012987012987\n",
      "\n",
      "Feature Importance:\n",
      "       Feature  Importance\n",
      "36     DMANEMP    0.058128\n",
      "31      PAYEMS    0.055808\n",
      "32      USGOOD    0.055329\n",
      "35      MANEMP    0.049528\n",
      "15   IPMANSICS    0.034101\n",
      "..         ...         ...\n",
      "21     CLF16OV    0.000994\n",
      "106   CPIULFSL    0.000981\n",
      "24    UEMPMEAN    0.000878\n",
      "68      REALLN    0.000846\n",
      "97   OILPRICEx    0.000520\n",
      "\n",
      "[119 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Nous allons créer des variables à t-1 pour chaque série temporelle\n",
    "fred_md_lagged = fred_md_short.shift(1)\n",
    "\n",
    "# Fusionner les données laggées avec l'indicateur de récession (us_rec) à l'index\n",
    "data = pd.concat([fred_md_lagged, us_rec], axis=1)\n",
    "\n",
    "# Définir les variables X (features) et y (target)\n",
    "X = data.drop(columns=['USREC'])  # Tout sauf 'us_rec' sera utilisé comme caractéristiques\n",
    "y = data['USREC']  # La variable cible est 'us_rec'\n",
    "\n",
    "# Run Random Forest with the custom function\n",
    "results = run_random_forest(X, y)\n",
    "\n",
    "# Display the results for 60% explained variance\n",
    "print(\"Best Hyperparameters :\", results['best_params'])\n",
    "print(\"\\nClassification Report:\")\n",
    "print(results['classification_report'])\n",
    "print(\"\\nAccuracy:\", results['accuracy'])\n",
    "print(\"\\nFeature Importance:\")\n",
    "print(results['feature_importance'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c6916b-6747-4f15-bd48-340378d1f878",
   "metadata": {},
   "source": [
    "Problèmes :\n",
    "- Récessions rares dans le dataset, donc le modèle a tout intérêt à prédire 0 s'il n'a pas assez d'informations sur l'état de l'économie\n",
    "- Trop de variables (besoin de PCA)\n",
    "- Besoin d'informations sur les variables en t-2..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21992be-b295-4993-84ab-a855804bf9f4",
   "metadata": {},
   "source": [
    "# RF with principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6afdc974-1f67-4332-b996-5ae2493fa5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data\n",
    "# Make sure the file paths are correct where you saved the PCA datasets\n",
    "pca_60_df = pd.read_csv('/home/onyxia/work/Macroeconometrics/data/PCA/pca_60.csv', index_col='sasdate')  # For 60% variance explained\n",
    "pca_80_df = pd.read_csv('/home/onyxia/work/Macroeconometrics/data/PCA/pca_80.csv', index_col='sasdate')  # For 80% variance explained\n",
    "pca_90_df = pd.read_csv('/home/onyxia/work/Macroeconometrics/data/PCA/pca_90.csv', index_col='sasdate')  # For 90% variance explained\n",
    "pca_60_df.index = pd.to_datetime(pca_60_df.index)\n",
    "pca_80_df.index = pd.to_datetime(pca_80_df.index)\n",
    "pca_90_df.index = pd.to_datetime(pca_90_df.index)\n",
    "\n",
    "pca_60_df = pca_60_df.shift(1)\n",
    "pca_80_df = pca_80_df.shift(1)\n",
    "pca_90_df = pca_90_df.shift(1)\n",
    "\n",
    "pca_60_df = pd.concat([pca_60_df, us_rec], axis=1, join='inner').dropna()\n",
    "pca_80_df = pd.concat([pca_80_df, us_rec], axis=1, join='inner').dropna()\n",
    "pca_90_df = pd.concat([pca_90_df, us_rec], axis=1, join='inner').dropna()\n",
    "\n",
    "# Target variable is 'USREC', the recession indicator\n",
    "X_60 = pca_60_df.drop(columns=['USREC'])\n",
    "y_60 = pca_60_df['USREC']\n",
    "\n",
    "X_80 = pca_80_df.drop(columns=['USREC'])\n",
    "y_80 = pca_80_df['USREC']\n",
    "\n",
    "X_90 = pca_90_df.drop(columns=['USREC'])\n",
    "y_90 = pca_90_df['USREC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "36d15a1f-3816-44dd-b83e-d2a2aeba3313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Best Hyperparameters for 60% Variance Explained: {'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 1000}\n",
      "\n",
      "Classification Report for 60% Variance Explained:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       152\n",
      "           1       0.33      0.50      0.40         2\n",
      "\n",
      "    accuracy                           0.98       154\n",
      "   macro avg       0.66      0.74      0.70       154\n",
      "weighted avg       0.98      0.98      0.98       154\n",
      "\n",
      "\n",
      "Accuracy for 60% Variance Explained: 0.9805194805194806\n",
      "\n",
      "Feature Importance for 60% Variance Explained:\n",
      "   Feature  Importance\n",
      "0      PC1    0.369943\n",
      "4      PC5    0.132299\n",
      "3      PC4    0.114240\n",
      "6      PC7    0.064705\n",
      "5      PC6    0.063077\n",
      "7      PC8    0.049956\n",
      "10    PC11    0.049557\n",
      "1      PC2    0.038172\n",
      "2      PC3    0.035285\n",
      "8      PC9    0.031370\n",
      "11    PC12    0.028793\n",
      "9     PC10    0.022604\n"
     ]
    }
   ],
   "source": [
    "# Run Random Forest with the custom function\n",
    "results_60 = run_random_forest(X_60, y_60)\n",
    "\n",
    "# Display the results for 60% explained variance\n",
    "print(\"Best Hyperparameters for 60% Variance Explained:\", results_60['best_params'])\n",
    "print(\"\\nClassification Report for 60% Variance Explained:\")\n",
    "print(results_60['classification_report'])\n",
    "print(\"\\nAccuracy for 60% Variance Explained:\", results_60['accuracy'])\n",
    "print(\"\\nFeature Importance for 60% Variance Explained:\")\n",
    "print(results_60['feature_importance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "514f3d2f-43db-4882-9eb3-35a2ef48c2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Best Hyperparameters for 80% Variance Explained: {'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 1000}\n",
      "\n",
      "Classification Report for 80% Variance Explained:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       152\n",
      "           1       0.33      0.50      0.40         2\n",
      "\n",
      "    accuracy                           0.98       154\n",
      "   macro avg       0.66      0.74      0.70       154\n",
      "weighted avg       0.98      0.98      0.98       154\n",
      "\n",
      "\n",
      "Accuracy for 80% Variance Explained: 0.9805194805194806\n",
      "\n",
      "Feature Importance for 80% Variance Explained:\n",
      "   Feature  Importance\n",
      "0      PC1    0.302469\n",
      "4      PC5    0.100981\n",
      "3      PC4    0.093508\n",
      "6      PC7    0.045948\n",
      "5      PC6    0.045179\n",
      "7      PC8    0.036975\n",
      "10    PC11    0.036197\n",
      "1      PC2    0.029107\n",
      "2      PC3    0.023551\n",
      "18    PC19    0.020423\n",
      "16    PC17    0.018347\n",
      "8      PC9    0.018067\n",
      "11    PC12    0.017614\n",
      "13    PC14    0.017000\n",
      "17    PC18    0.016166\n",
      "12    PC13    0.016160\n",
      "19    PC20    0.014494\n",
      "27    PC28    0.014463\n",
      "25    PC26    0.014456\n",
      "23    PC24    0.014455\n",
      "15    PC16    0.013840\n",
      "21    PC22    0.013338\n",
      "9     PC10    0.012141\n",
      "14    PC15    0.012019\n",
      "24    PC25    0.011873\n",
      "28    PC29    0.011477\n",
      "20    PC21    0.011201\n",
      "22    PC23    0.009369\n",
      "26    PC27    0.009179\n"
     ]
    }
   ],
   "source": [
    "# Run Random Forest with the custom function\n",
    "results_80 = run_random_forest(X_80, y_80)\n",
    "\n",
    "# Display the results for 80% explained variance\n",
    "print(\"Best Hyperparameters for 80% Variance Explained:\", results_80['best_params'])\n",
    "print(\"\\nClassification Report for 80% Variance Explained:\")\n",
    "print(results_80['classification_report'])\n",
    "print(\"\\nAccuracy for 80% Variance Explained:\", results_80['accuracy'])\n",
    "print(\"\\nFeature Importance for 80% Variance Explained:\")\n",
    "print(results_80['feature_importance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d6a112e6-3ef4-48d0-9f73-b14046503d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Best Hyperparameters for 90% Variance Explained: {'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 1000}\n",
      "\n",
      "Classification Report for 90% Variance Explained:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99       152\n",
      "           1       0.25      0.50      0.33         2\n",
      "\n",
      "    accuracy                           0.97       154\n",
      "   macro avg       0.62      0.74      0.66       154\n",
      "weighted avg       0.98      0.97      0.98       154\n",
      "\n",
      "\n",
      "Accuracy for 90% Variance Explained: 0.974025974025974\n",
      "\n",
      "Feature Importance for 90% Variance Explained:\n",
      "   Feature  Importance\n",
      "0      PC1    0.261004\n",
      "3      PC4    0.090884\n",
      "4      PC5    0.090346\n",
      "5      PC6    0.039208\n",
      "6      PC7    0.038643\n",
      "7      PC8    0.031387\n",
      "10    PC11    0.030516\n",
      "1      PC2    0.024664\n",
      "2      PC3    0.019175\n",
      "18    PC19    0.017811\n",
      "11    PC12    0.015678\n",
      "37    PC38    0.015400\n",
      "43    PC44    0.015258\n",
      "40    PC41    0.014994\n",
      "12    PC13    0.014500\n",
      "16    PC17    0.014444\n",
      "17    PC18    0.014108\n",
      "8      PC9    0.013456\n",
      "13    PC14    0.012827\n",
      "41    PC42    0.012220\n",
      "35    PC36    0.012049\n",
      "38    PC39    0.011951\n",
      "23    PC24    0.010751\n",
      "19    PC20    0.010340\n",
      "26    PC27    0.010284\n",
      "31    PC32    0.010067\n",
      "25    PC26    0.010058\n",
      "21    PC22    0.009962\n",
      "15    PC16    0.009863\n",
      "9     PC10    0.009723\n",
      "36    PC37    0.009524\n",
      "20    PC21    0.009358\n",
      "42    PC43    0.008718\n",
      "24    PC25    0.008429\n",
      "22    PC23    0.008287\n",
      "14    PC15    0.008220\n",
      "27    PC28    0.008010\n",
      "32    PC33    0.007834\n",
      "30    PC31    0.007195\n",
      "28    PC29    0.006876\n",
      "34    PC35    0.006740\n",
      "39    PC40    0.006545\n",
      "33    PC34    0.006450\n",
      "29    PC30    0.006243\n"
     ]
    }
   ],
   "source": [
    "# Run Random Forest with the custom function\n",
    "results_90 = run_random_forest(X_90, y_90)\n",
    "\n",
    "# Display the results for 90% explained variance\n",
    "print(\"Best Hyperparameters for 90% Variance Explained:\", results_90['best_params'])\n",
    "print(\"\\nClassification Report for 90% Variance Explained:\")\n",
    "print(results_90['classification_report'])\n",
    "print(\"\\nAccuracy for 90% Variance Explained:\", results_90['accuracy'])\n",
    "print(\"\\nFeature Importance for 90% Variance Explained:\")\n",
    "print(results_90['feature_importance'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc00278-41ab-4182-bcfd-b260e56e4013",
   "metadata": {},
   "source": [
    "# RF avec PCA pour 60% en dupliquant les périodes en récession "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f017e130-625a-47ba-9404-2609d1349fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data_60_dup_2x: (863, 13)\n",
      "Shape of data_60_dup_5x: (1148, 13)\n",
      "Shape of data_60_dup_10x: (1623, 13)\n"
     ]
    }
   ],
   "source": [
    "# Fonction pour dupliquer les lignes avec une condition\n",
    "def duplicate_recession(data, factor=2):\n",
    "    # Sélectionner les lignes où 'USREC' == 1 (récession)\n",
    "    recession_data = data[data['USREC'] == 1]\n",
    "    # Sélectionner les lignes où 'USREC' == 0 (non-récession)\n",
    "    non_recession_data = data[data['USREC'] == 0]\n",
    "    \n",
    "    # Dupliquer les lignes de récession par un facteur donné\n",
    "    duplicated_recession = pd.concat([recession_data] * factor, axis=0, ignore_index=True)\n",
    "    \n",
    "    # Concatenner les récessions dupliquées avec les périodes non-récession\n",
    "    new_data = pd.concat([non_recession_data, duplicated_recession], axis=0, ignore_index=True)\n",
    "    \n",
    "    return new_data\n",
    "\n",
    "# Appliquer la duplication des récessions par un facteur de 2, 5 et 10\n",
    "data_60_dup_2x = duplicate_recession(pca_60_df, factor=2)\n",
    "data_60_dup_5x = duplicate_recession(pca_60_df, factor=5)\n",
    "data_60_dup_10x = duplicate_recession(pca_60_df, factor=10)\n",
    "\n",
    "# Afficher la taille des nouveaux DataFrames pour vérifier\n",
    "print(\"Shape of data_60_dup_2x:\", data_60_dup_2x.shape)\n",
    "print(\"Shape of data_60_dup_5x:\", data_60_dup_5x.shape)\n",
    "print(\"Shape of data_60_dup_10x:\", data_60_dup_10x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0d79abe1-96f1-4e0d-8a27-33d6ad2e116d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "\n",
      "Results for 2x duplicated recession periods:\n",
      "Best Hyperparameters: {'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 1000}\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       1.00      0.03      0.06       173\n",
      "\n",
      "    accuracy                           0.03       173\n",
      "   macro avg       0.50      0.01      0.03       173\n",
      "weighted avg       1.00      0.03      0.06       173\n",
      "\n",
      "Accuracy: 0.028901734104046242\n",
      "\n",
      "Results for 5x duplicated recession periods:\n",
      "Best Hyperparameters: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 1000}\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00       230\n",
      "\n",
      "    accuracy                           1.00       230\n",
      "   macro avg       1.00      1.00      1.00       230\n",
      "weighted avg       1.00      1.00      1.00       230\n",
      "\n",
      "Accuracy: 1.0\n",
      "\n",
      "Results for 10x duplicated recession periods:\n",
      "Best Hyperparameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 1000}\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00       325\n",
      "\n",
      "    accuracy                           1.00       325\n",
      "   macro avg       1.00      1.00      1.00       325\n",
      "weighted avg       1.00      1.00      1.00       325\n",
      "\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Now we will use the `run_random_forest` function on the modified datasets\n",
    "# Prepare data for 60% explained variance (after duplication)\n",
    "X_60_dup_2x = data_60_dup_2x.drop(columns=['USREC'])\n",
    "y_60_dup_2x = data_60_dup_2x['USREC']\n",
    "results_60_dup_2x = run_random_forest(X_60_dup_2x, y_60_dup_2x)\n",
    "\n",
    "X_60_dup_5x = data_60_dup_5x.drop(columns=['USREC'])\n",
    "y_60_dup_5x = data_60_dup_5x['USREC']\n",
    "results_60_dup_5x = run_random_forest(X_60_dup_5x, y_60_dup_5x)\n",
    "\n",
    "X_60_dup_10x = data_60_dup_10x.drop(columns=['USREC'])\n",
    "y_60_dup_10x = data_60_dup_10x['USREC']\n",
    "results_60_dup_10x = run_random_forest(X_60_dup_10x, y_60_dup_10x)\n",
    "\n",
    "# Displaying the results\n",
    "print(\"\\nResults for 2x duplicated recession periods:\")\n",
    "print(\"Best Hyperparameters:\", results_60_dup_2x['best_params'])\n",
    "print(\"Classification Report:\\n\", results_60_dup_2x['classification_report'])\n",
    "print(\"Accuracy:\", results_60_dup_2x['accuracy'])\n",
    "\n",
    "print(\"\\nResults for 5x duplicated recession periods:\")\n",
    "print(\"Best Hyperparameters:\", results_60_dup_5x['best_params'])\n",
    "print(\"Classification Report:\\n\", results_60_dup_5x['classification_report'])\n",
    "print(\"Accuracy:\", results_60_dup_5x['accuracy'])\n",
    "\n",
    "print(\"\\nResults for 10x duplicated recession periods:\")\n",
    "print(\"Best Hyperparameters:\", results_60_dup_10x['best_params'])\n",
    "print(\"Classification Report:\\n\", results_60_dup_10x['classification_report'])\n",
    "print(\"Accuracy:\", results_60_dup_10x['accuracy'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
