{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6a138935-1a32-411e-9de1-0f5bf8d74029",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Librairies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import contextlib\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/onyxia/work/Macroeconometrics')\n",
    "from src.preprocessing import apply_transformation\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2729921f-b723-4ea3-8000-49bdb290f69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main data\n",
    "fred_md = pd.read_csv(\"/home/onyxia/work/Macroeconometrics/data/fred_md_2024_12.csv\")\n",
    "#Metadata\n",
    "fred_info = pd.read_csv(\"/home/onyxia/work/Macroeconometrics/data/FRED_MD_updated_appendix.csv\", encoding=\"latin1\")\n",
    "#Recession variable\n",
    "us_rec = pd.read_csv(\"/home/onyxia/work/Macroeconometrics/data/USREC.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5aa3d16c-7430-48a6-a18b-4e4a8e462a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Indexing the dataset with dates\n",
    "fred_md_short = (\n",
    "    fred_md.iloc[1:]\n",
    "    .assign(sasdate=pd.to_datetime(fred_md.iloc[1:].sasdate, format=\"%m/%d/%Y\"))\n",
    "    .set_index(\"sasdate\")\n",
    ")\n",
    "\n",
    "#Transformation of series based on metadata\n",
    "for _, row in fred_info.iterrows():\n",
    "    series_name = row['fred']\n",
    "    transformation_code = row['tcode']\n",
    "\n",
    "    with contextlib.suppress(Exception):\n",
    "        fred_md_short[series_name] = apply_transformation(fred_md_short[series_name], transformation_code)\n",
    "\n",
    "#Filtering data by date\n",
    "start_date = \"1960\"\n",
    "end_date = \"2024\"\n",
    "fred_md_short = fred_md_short[\n",
    "    (fred_md_short.index >= start_date) & (fred_md_short.index <= end_date)\n",
    "].dropna(axis=1)\n",
    "\n",
    "#Addition of the variable of interest (American recession)\n",
    "us_rec = us_rec.assign(\n",
    "    observation_date=pd.to_datetime(us_rec.observation_date)\n",
    ").set_index(\"observation_date\")\n",
    "us_rec = us_rec.loc[fred_md_short.index,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "29ac2d87-9add-4935-9b63-d9e5272c94b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function Random Forest\n",
    "def run_random_forest(X, y, param_grid=None, test_size=0.2, random_state=667, cv_folds=5):\n",
    "    \"\"\"\n",
    "    Perform Random Forest classification with hyperparameter tuning using GridSearchCV.\n",
    "\n",
    "    Parameters:\n",
    "    X : Covariates (features) for training the model.\n",
    "    y : Target variable for classification.\n",
    "    param_grid : Dictionary of hyperparameters to tune.\n",
    "    test_size : Fraction of data to use for testing (default is 0.2).\n",
    "    random_state : Random seed for reproducibility (default is 42).\n",
    "    cv_folds : Number of folds for cross-validation (default is 5).\n",
    "    \n",
    "    Returns:\n",
    "    dict: Contains 'best_params', 'classification_report', 'accuracy', and 'feature_importance'.\n",
    "    \"\"\"\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Split the data into train and test sets (time-series split without shuffling)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=test_size, random_state=random_state, stratify = y)\n",
    "    \n",
    "    # Instantiate Random Forest model\n",
    "    rf = RandomForestClassifier(random_state=random_state)\n",
    "    \n",
    "    # Hyperparameter tuning using GridSearchCV\n",
    "    if param_grid is None:\n",
    "        param_grid = {\n",
    "        'n_estimators': [1000],  # Nombre d'arbres dans la forêt\n",
    "        'max_depth': [None, 10, 30],   # Profondeur maximale des arbres\n",
    "        'min_samples_split': [2, 5, 10],   # Nombre minimal d'échantillons pour diviser un noeud\n",
    "        'min_samples_leaf': [1, 4],     # Nombre minimal d'échantillons par feuille\n",
    "        }\n",
    "    \n",
    "    grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=cv_folds, n_jobs=-1, verbose=2)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    #Best model\n",
    "    best_rf = grid_search.best_estimator_\n",
    "\n",
    "    #Predictions on the test set\n",
    "    y_pred = best_rf.predict(X_test)\n",
    "\n",
    "    #Evaluation\n",
    "    classification_rep = classification_report(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    #Feature importance\n",
    "    importances = best_rf.feature_importances_\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'Feature': X.columns if isinstance(X, pd.DataFrame) else [f'PC{i+1}' for i in range(X.shape[1])],\n",
    "        'Importance': importances\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "    # Return results\n",
    "    return {\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'classification_report': classification_rep,\n",
    "        'accuracy': accuracy,\n",
    "        'feature_importance': feature_importance_df\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf44473-f241-473a-b35a-a3c1da05a3ca",
   "metadata": {},
   "source": [
    "# RF naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "dacfaf60-1461-44b5-953c-0e86a3b0695b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Best Hyperparameters : {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 1000}\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.98      0.96       135\n",
      "           1       0.80      0.63      0.71        19\n",
      "\n",
      "    accuracy                           0.94       154\n",
      "   macro avg       0.87      0.80      0.83       154\n",
      "weighted avg       0.93      0.94      0.93       154\n",
      "\n",
      "\n",
      "Accuracy: 0.935064935064935\n",
      "\n",
      "Feature Importance:\n",
      "           Feature  Importance\n",
      "31          PAYEMS    0.057510\n",
      "32          USGOOD    0.057091\n",
      "36         DMANEMP    0.044140\n",
      "35          MANEMP    0.041427\n",
      "82        TB3SMFFM    0.029373\n",
      "..             ...         ...\n",
      "21         CLF16OV    0.001831\n",
      "107  CUSR0000SA0L2    0.001803\n",
      "89         EXSZUSx    0.001770\n",
      "45         AWOTMAN    0.001588\n",
      "97       OILPRICEx    0.001414\n",
      "\n",
      "[119 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Nous allons créer des variables à t-1 pour chaque série temporelle\n",
    "fred_md_lagged = fred_md_short.shift(1)\n",
    "\n",
    "# Fusionner les données laggées avec l'indicateur de récession (us_rec) à l'index\n",
    "data = pd.concat([fred_md_lagged, us_rec], axis=1)\n",
    "\n",
    "# Définir les variables X (features) et y (target)\n",
    "X = data.drop(columns=['USREC'])  # Tout sauf 'us_rec' sera utilisé comme caractéristiques\n",
    "y = data['USREC']  # La variable cible est 'us_rec'\n",
    "\n",
    "# Run Random Forest with the custom function\n",
    "results = run_random_forest(X, y)\n",
    "\n",
    "# Display the results \n",
    "print(\"Best Hyperparameters :\", results['best_params'])\n",
    "print(\"\\nClassification Report:\")\n",
    "print(results['classification_report'])\n",
    "print(\"\\nAccuracy:\", results['accuracy'])\n",
    "print(\"\\nFeature Importance:\")\n",
    "print(results['feature_importance'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c6916b-6747-4f15-bd48-340378d1f878",
   "metadata": {},
   "source": [
    "Problèmes :\n",
    "- Récessions rares dans le dataset, donc le modèle a tout intérêt à prédire 0 s'il n'a pas assez d'informations sur l'état de l'économie\n",
    "- Trop de variables (besoin de PCA)\n",
    "- Besoin d'informations sur les variables en t-2..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21992be-b295-4993-84ab-a855804bf9f4",
   "metadata": {},
   "source": [
    "# RF with principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "6afdc974-1f67-4332-b996-5ae2493fa5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data\n",
    "# Make sure the file paths are correct where you saved the PCA datasets\n",
    "pca_60_df = pd.read_csv('/home/onyxia/work/Macroeconometrics/data/PCA/pca_60.csv', index_col='sasdate')  # For 60% variance explained\n",
    "pca_80_df = pd.read_csv('/home/onyxia/work/Macroeconometrics/data/PCA/pca_80.csv', index_col='sasdate')  # For 80% variance explained\n",
    "pca_90_df = pd.read_csv('/home/onyxia/work/Macroeconometrics/data/PCA/pca_90.csv', index_col='sasdate')  # For 90% variance explained\n",
    "pca_60_df.index = pd.to_datetime(pca_60_df.index)\n",
    "pca_80_df.index = pd.to_datetime(pca_80_df.index)\n",
    "pca_90_df.index = pd.to_datetime(pca_90_df.index)\n",
    "\n",
    "pca_60_df = pca_60_df.shift(1)\n",
    "pca_80_df = pca_80_df.shift(1)\n",
    "pca_90_df = pca_90_df.shift(1)\n",
    "\n",
    "pca_60_df = pd.concat([pca_60_df, us_rec], axis=1, join='inner').dropna()\n",
    "pca_80_df = pd.concat([pca_80_df, us_rec], axis=1, join='inner').dropna()\n",
    "pca_90_df = pd.concat([pca_90_df, us_rec], axis=1, join='inner').dropna()\n",
    "\n",
    "# Target variable is 'USREC', the recession indicator\n",
    "X_60 = pca_60_df.drop(columns=['USREC'])\n",
    "y_60 = pca_60_df['USREC']\n",
    "\n",
    "X_80 = pca_80_df.drop(columns=['USREC'])\n",
    "y_80 = pca_80_df['USREC']\n",
    "\n",
    "X_90 = pca_90_df.drop(columns=['USREC'])\n",
    "y_90 = pca_90_df['USREC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "36d15a1f-3816-44dd-b83e-d2a2aeba3313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Best Hyperparameters for 60% Variance Explained: {'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 1000}\n",
      "\n",
      "Classification Report for 60% Variance Explained:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.97       135\n",
      "           1       0.92      0.58      0.71        19\n",
      "\n",
      "    accuracy                           0.94       154\n",
      "   macro avg       0.93      0.79      0.84       154\n",
      "weighted avg       0.94      0.94      0.94       154\n",
      "\n",
      "\n",
      "Accuracy for 60% Variance Explained: 0.9415584415584416\n",
      "\n",
      "Feature Importance for 60% Variance Explained:\n",
      "   Feature  Importance\n",
      "0      PC1    0.378559\n",
      "4      PC5    0.115435\n",
      "3      PC4    0.109692\n",
      "5      PC6    0.077943\n",
      "6      PC7    0.063657\n",
      "10    PC11    0.053354\n",
      "2      PC3    0.040001\n",
      "1      PC2    0.038192\n",
      "7      PC8    0.034314\n",
      "11    PC12    0.032062\n",
      "9     PC10    0.028751\n",
      "8      PC9    0.028042\n"
     ]
    }
   ],
   "source": [
    "# Run Random Forest with the custom function\n",
    "results_60 = run_random_forest(X_60, y_60)\n",
    "\n",
    "# Display the results for 60% explained variance\n",
    "print(\"Best Hyperparameters for 60% Variance Explained:\", results_60['best_params'])\n",
    "print(\"\\nClassification Report for 60% Variance Explained:\")\n",
    "print(results_60['classification_report'])\n",
    "print(\"\\nAccuracy for 60% Variance Explained:\", results_60['accuracy'])\n",
    "print(\"\\nFeature Importance for 60% Variance Explained:\")\n",
    "print(results_60['feature_importance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "514f3d2f-43db-4882-9eb3-35a2ef48c2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Best Hyperparameters for 80% Variance Explained: {'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 1000}\n",
      "\n",
      "Classification Report for 80% Variance Explained:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.99      0.96       135\n",
      "           1       0.90      0.47      0.62        19\n",
      "\n",
      "    accuracy                           0.93       154\n",
      "   macro avg       0.92      0.73      0.79       154\n",
      "weighted avg       0.93      0.93      0.92       154\n",
      "\n",
      "\n",
      "Accuracy for 80% Variance Explained: 0.9285714285714286\n",
      "\n",
      "Feature Importance for 80% Variance Explained:\n",
      "   Feature  Importance\n",
      "0      PC1    0.306997\n",
      "3      PC4    0.091000\n",
      "4      PC5    0.086481\n",
      "5      PC6    0.057627\n",
      "6      PC7    0.046589\n",
      "10    PC11    0.033723\n",
      "1      PC2    0.025870\n",
      "2      PC3    0.025371\n",
      "12    PC13    0.022099\n",
      "18    PC19    0.021832\n",
      "7      PC8    0.021311\n",
      "11    PC12    0.019797\n",
      "13    PC14    0.019462\n",
      "27    PC28    0.017394\n",
      "17    PC18    0.016827\n",
      "16    PC17    0.016489\n",
      "21    PC22    0.016247\n",
      "20    PC21    0.016065\n",
      "19    PC20    0.015219\n",
      "14    PC15    0.014702\n",
      "9     PC10    0.014155\n",
      "23    PC24    0.013525\n",
      "8      PC9    0.013312\n",
      "24    PC25    0.012738\n",
      "25    PC26    0.012554\n",
      "26    PC27    0.011329\n",
      "22    PC23    0.011294\n",
      "15    PC16    0.010519\n",
      "28    PC29    0.009473\n"
     ]
    }
   ],
   "source": [
    "# Run Random Forest with the custom function\n",
    "results_80 = run_random_forest(X_80, y_80)\n",
    "\n",
    "# Display the results for 80% explained variance\n",
    "print(\"Best Hyperparameters for 80% Variance Explained:\", results_80['best_params'])\n",
    "print(\"\\nClassification Report for 80% Variance Explained:\")\n",
    "print(results_80['classification_report'])\n",
    "print(\"\\nAccuracy for 80% Variance Explained:\", results_80['accuracy'])\n",
    "print(\"\\nFeature Importance for 80% Variance Explained:\")\n",
    "print(results_80['feature_importance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d6a112e6-3ef4-48d0-9f73-b14046503d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Best Hyperparameters for 90% Variance Explained: {'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 1000}\n",
      "\n",
      "Classification Report for 90% Variance Explained:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.99      0.96       135\n",
      "           1       0.90      0.47      0.62        19\n",
      "\n",
      "    accuracy                           0.93       154\n",
      "   macro avg       0.92      0.73      0.79       154\n",
      "weighted avg       0.93      0.93      0.92       154\n",
      "\n",
      "\n",
      "Accuracy for 90% Variance Explained: 0.9285714285714286\n",
      "\n",
      "Feature Importance for 90% Variance Explained:\n",
      "   Feature  Importance\n",
      "0      PC1    0.257344\n",
      "3      PC4    0.082402\n",
      "4      PC5    0.079797\n",
      "5      PC6    0.050609\n",
      "6      PC7    0.038523\n",
      "10    PC11    0.027078\n",
      "2      PC3    0.019900\n",
      "43    PC44    0.019210\n",
      "1      PC2    0.018905\n",
      "12    PC13    0.018436\n",
      "18    PC19    0.018011\n",
      "11    PC12    0.017631\n",
      "7      PC8    0.016107\n",
      "31    PC32    0.016034\n",
      "13    PC14    0.014921\n",
      "17    PC18    0.014687\n",
      "38    PC39    0.014660\n",
      "26    PC27    0.014036\n",
      "35    PC36    0.013989\n",
      "37    PC38    0.013488\n",
      "20    PC21    0.013251\n",
      "40    PC41    0.013133\n",
      "23    PC24    0.013066\n",
      "9     PC10    0.012955\n",
      "16    PC17    0.012535\n",
      "36    PC37    0.011915\n",
      "42    PC43    0.010929\n",
      "8      PC9    0.010915\n",
      "30    PC31    0.010549\n",
      "19    PC20    0.010526\n",
      "14    PC15    0.010511\n",
      "41    PC42    0.010117\n",
      "21    PC22    0.009806\n",
      "15    PC16    0.009297\n",
      "22    PC23    0.008589\n",
      "27    PC28    0.008102\n",
      "24    PC25    0.008095\n",
      "25    PC26    0.007976\n",
      "32    PC33    0.007801\n",
      "29    PC30    0.007756\n",
      "34    PC35    0.007542\n",
      "39    PC40    0.007494\n",
      "28    PC29    0.006075\n",
      "33    PC34    0.005296\n"
     ]
    }
   ],
   "source": [
    "# Run Random Forest with the custom function\n",
    "results_90 = run_random_forest(X_90, y_90)\n",
    "\n",
    "# Display the results for 90% explained variance\n",
    "print(\"Best Hyperparameters for 90% Variance Explained:\", results_90['best_params'])\n",
    "print(\"\\nClassification Report for 90% Variance Explained:\")\n",
    "print(results_90['classification_report'])\n",
    "print(\"\\nAccuracy for 90% Variance Explained:\", results_90['accuracy'])\n",
    "print(\"\\nFeature Importance for 90% Variance Explained:\")\n",
    "print(results_90['feature_importance'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933e6a1c-fff9-489e-83a7-631beaef3d9d",
   "metadata": {},
   "source": [
    "# RF avec sparse factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "1c4b433e-098d-4663-888b-39f5871468e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Best Hyperparameters: {'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 1000}\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.98      0.96       135\n",
      "           1       0.80      0.63      0.71        19\n",
      "\n",
      "    accuracy                           0.94       154\n",
      "   macro avg       0.87      0.80      0.83       154\n",
      "weighted avg       0.93      0.94      0.93       154\n",
      "\n",
      "\n",
      "Accuracy : 0.935064935064935\n",
      "\n",
      "Feature Importance :\n",
      "  Feature  Importance\n",
      "0       0    0.446972\n",
      "7       7    0.097172\n",
      "3       3    0.069335\n",
      "6       6    0.067485\n",
      "4       4    0.064086\n",
      "1       1    0.061481\n",
      "9       9    0.059798\n",
      "5       5    0.047857\n",
      "2       2    0.043119\n",
      "8       8    0.042695\n"
     ]
    }
   ],
   "source": [
    "factors = pd.read_csv('/home/onyxia/work/Macroeconometrics/data/estimated_factor.csv', index_col='sasdate') \n",
    "factors.index = pd.to_datetime(factors.index)\n",
    "factors = factors.shift(1)\n",
    "\n",
    "\n",
    "factors = pd.concat([factors, us_rec], axis=1, join='inner').dropna()\n",
    "\n",
    "# Target variable is 'USREC', the recession indicator\n",
    "X_factors = factors.drop(columns=['USREC'])\n",
    "y_factors = factors['USREC']\n",
    "\n",
    "# Run Random Forest with the custom function\n",
    "results_factors = run_random_forest(X_factors, y_factors)\n",
    "\n",
    "# Display the results \n",
    "print(\"Best Hyperparameters:\", results_factors['best_params'])\n",
    "print(\"\\nClassification Report:\")\n",
    "print(results_factors['classification_report'])\n",
    "print(\"\\nAccuracy :\", results_factors['accuracy'])\n",
    "print(\"\\nFeature Importance :\")\n",
    "print(results_factors['feature_importance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "083279b5-603d-42a4-a61c-9376f0067707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Best Hyperparameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 1000}\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.96       135\n",
      "           1       0.85      0.58      0.69        19\n",
      "\n",
      "    accuracy                           0.94       154\n",
      "   macro avg       0.89      0.78      0.83       154\n",
      "weighted avg       0.93      0.94      0.93       154\n",
      "\n",
      "\n",
      "Accuracy : 0.935064935064935\n",
      "\n",
      "Feature Importance :\n",
      "         Feature  Importance\n",
      "0              0    0.062357\n",
      "41        PAYEMS    0.045057\n",
      "46       DMANEMP    0.043257\n",
      "42        USGOOD    0.040794\n",
      "45        MANEMP    0.039101\n",
      "..           ...         ...\n",
      "72          M2SL    0.001794\n",
      "119        PCEPI    0.001794\n",
      "55       AWOTMAN    0.001702\n",
      "126  DTCOLNVHFNM    0.001514\n",
      "113  CUSR0000SAC    0.001382\n",
      "\n",
      "[129 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#on combine les sparses factors à toutes les covariates (fred_md_short)\n",
    "factors = pd.read_csv('/home/onyxia/work/Macroeconometrics/data/estimated_factor.csv', index_col='sasdate') \n",
    "factors.index = pd.to_datetime(factors.index)\n",
    "\n",
    "data = pd.merge(factors, fred_md_short, left_index=True, right_index=True, how='inner')\n",
    "data = data.shift(1)\n",
    "\n",
    "data = pd.concat([data, us_rec], axis=1, join='inner').dropna()\n",
    "\n",
    "# Target variable is 'USREC', the recession indicator\n",
    "X = data.drop(columns=['USREC'])\n",
    "y = data['USREC']\n",
    "\n",
    "# Run Random Forest with the custom function\n",
    "results = run_random_forest(X, y)\n",
    "\n",
    "# Display the results \n",
    "print(\"Best Hyperparameters:\", results['best_params'])\n",
    "print(\"\\nClassification Report:\")\n",
    "print(results['classification_report'])\n",
    "print(\"\\nAccuracy :\", results['accuracy'])\n",
    "print(\"\\nFeature Importance :\")\n",
    "print(results['feature_importance'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc00278-41ab-4182-bcfd-b260e56e4013",
   "metadata": {},
   "source": [
    "# RF avec factors et covariates en dupliquant les périodes de récession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f017e130-625a-47ba-9404-2609d1349fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fonction de duplication des lignes ayant une récession\n",
    "def duplicate_usrec(data, k):\n",
    "    # Filtrer les lignes où USREC == 1\n",
    "    usrec_1 = data[data['USREC'] == 1]\n",
    "    usrec_0 = data[data['USREC'] == 0]\n",
    "    \n",
    "    # Dupliquer les lignes k fois\n",
    "    duplicated_rows = pd.concat([usrec_1] * k, ignore_index=True)\n",
    "    \n",
    "    # Ajouter les lignes dupliquées au DataFrame d'origine\n",
    "    data_with_duplicates = pd.concat([usrec_0, duplicated_rows], ignore_index=True)\n",
    "\n",
    "    # Mélanger aléatoirement les lignes\n",
    "    data_with_duplicates = data_with_duplicates.sample(frac=1, random_state=667).reset_index(drop=True)\n",
    "    \n",
    "    return data_with_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c26081cf-8e36-4b72-9673-187ee6aae7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   1.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   1.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   1.9s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   1.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=1000; total time=   1.6s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=1000; total time=   1.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   1.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   1.8s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=   1.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=1000; total time=   1.8s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   2.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   1.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   1.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   1.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   1.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   1.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   1.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=1000; total time=   1.8s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=   1.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=   1.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=1000; total time=   1.8s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=1000; total time=   2.0s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   1.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=1000; total time=   1.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=1000; total time=   1.6s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   1.6s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=1000; total time=   1.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   1.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=1000; total time=   1.8s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   1.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   1.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=1000; total time=   1.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=1000; total time=   1.8s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=1000; total time=   1.8s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=   1.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=   1.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   2.0s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=   1.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=   2.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=   2.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   1.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=1000; total time=   1.8s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=   1.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=1000; total time=   1.8s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=   3.8s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=1000; total time=   3.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   3.6s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   4.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   1.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=1000; total time=   1.6s[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   1.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=   1.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=1000; total time=   1.8s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   1.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=   1.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=1000; total time=   1.6s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=1000; total time=   1.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=   1.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=1000; total time=   1.8s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   1.8s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   1.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=1000; total time=   1.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=   2.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   1.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   1.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   1.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=   1.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   1.8s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=1000; total time=   1.6s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=1000; total time=   1.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   2.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=1000; total time=   1.9s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   1.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=   1.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=1000; total time=   1.6s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   1.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=   1.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   2.0s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   1.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=1000; total time=   1.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=1000; total time=   1.8s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=1000; total time=   1.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=1000; total time=   1.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   1.9s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=   1.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   2.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=1000; total time=   1.9s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   1.8s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   1.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=   1.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=1000; total time=   1.9s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   1.8s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   1.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=   1.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=1000; total time=   1.8s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=1000; total time=   1.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=   4.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   3.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=1000; total time=   3.5s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=1000; total time=   3.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=   1.6s[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=10, n_estimators=1000; total time=   1.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   1.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=1000; total time=   1.6s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=1000; total time=   1.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=1000; total time=   1.9s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   1.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   1.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   1.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=1000; total time=   1.6s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=   2.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   1.8s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   1.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=1000; total time=   1.6s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   1.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   1.8s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=5, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   1.8s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=1000; total time=   1.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   1.9s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=   2.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=1000; total time=   1.8s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   1.8s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   1.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   1.6s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   1.6s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=1000; total time=   1.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   1.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=1000; total time=   1.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=   1.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   1.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=1000; total time=   2.9s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=5, n_estimators=1000; total time=   1.9s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   2.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   1.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=10, n_estimators=1000; total time=   1.9s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   2.0s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   3.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=10, n_estimators=1000; total time=   1.9s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=1000; total time=   1.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   1.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=1000; total time=   1.8s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=10, n_estimators=1000; total time=   1.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   2.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   1.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=1000; total time=   1.9s\n",
      "[CV] END max_depth=30, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=4, min_samples_split=10, n_estimators=1000; total time=   1.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   1.8s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=   3.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=   3.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=1000; total time=   3.5s\n",
      "[CV] END max_depth=30, min_samples_leaf=1, min_samples_split=10, n_estimators=1000; total time=   3.9s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=1000; total time=   1.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=1000; total time=   1.6s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de duplications : 1\n",
      "Best Hyperparameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 1000}\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.93       135\n",
      "           1       0.00      0.00      0.00        19\n",
      "\n",
      "    accuracy                           0.88       154\n",
      "   macro avg       0.44      0.50      0.47       154\n",
      "weighted avg       0.77      0.88      0.82       154\n",
      "\n",
      "\n",
      "Accuracy : 0.8766233766233766\n",
      "\n",
      "Feature Importance :\n",
      "           Feature  Importance\n",
      "0                0         0.0\n",
      "1                1         0.0\n",
      "2                2         0.0\n",
      "3                3         0.0\n",
      "4                4         0.0\n",
      "..             ...         ...\n",
      "124  CES2000000008         0.0\n",
      "125  CES3000000008         0.0\n",
      "126    DTCOLNVHFNM         0.0\n",
      "127       DTCTHFNM         0.0\n",
      "128         INVEST         0.0\n",
      "\n",
      "[129 rows x 2 columns]\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de duplications : 4\n",
      "Best Hyperparameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 1000}\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.93       135\n",
      "           1       0.00      0.00      0.00        19\n",
      "\n",
      "    accuracy                           0.88       154\n",
      "   macro avg       0.44      0.50      0.47       154\n",
      "weighted avg       0.77      0.88      0.82       154\n",
      "\n",
      "\n",
      "Accuracy : 0.8766233766233766\n",
      "\n",
      "Feature Importance :\n",
      "           Feature  Importance\n",
      "0                0         0.0\n",
      "1                1         0.0\n",
      "2                2         0.0\n",
      "3                3         0.0\n",
      "4                4         0.0\n",
      "..             ...         ...\n",
      "124  CES2000000008         0.0\n",
      "125  CES3000000008         0.0\n",
      "126    DTCOLNVHFNM         0.0\n",
      "127       DTCTHFNM         0.0\n",
      "128         INVEST         0.0\n",
      "\n",
      "[129 rows x 2 columns]\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de duplications : 7\n",
      "Best Hyperparameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 1000}\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.93       135\n",
      "           1       0.00      0.00      0.00        19\n",
      "\n",
      "    accuracy                           0.88       154\n",
      "   macro avg       0.44      0.50      0.47       154\n",
      "weighted avg       0.77      0.88      0.82       154\n",
      "\n",
      "\n",
      "Accuracy : 0.8766233766233766\n",
      "\n",
      "Feature Importance :\n",
      "           Feature  Importance\n",
      "0                0         0.0\n",
      "1                1         0.0\n",
      "2                2         0.0\n",
      "3                3         0.0\n",
      "4                4         0.0\n",
      "..             ...         ...\n",
      "124  CES2000000008         0.0\n",
      "125  CES3000000008         0.0\n",
      "126    DTCOLNVHFNM         0.0\n",
      "127       DTCTHFNM         0.0\n",
      "128         INVEST         0.0\n",
      "\n",
      "[129 rows x 2 columns]\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de duplications : 10\n",
      "Best Hyperparameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 1000}\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       135\n",
      "           1       0.12      1.00      0.22        19\n",
      "\n",
      "    accuracy                           0.12       154\n",
      "   macro avg       0.06      0.50      0.11       154\n",
      "weighted avg       0.02      0.12      0.03       154\n",
      "\n",
      "\n",
      "Accuracy : 0.12337662337662338\n",
      "\n",
      "Feature Importance :\n",
      "           Feature  Importance\n",
      "0                0         0.0\n",
      "1                1         0.0\n",
      "2                2         0.0\n",
      "3                3         0.0\n",
      "4                4         0.0\n",
      "..             ...         ...\n",
      "124  CES2000000008         0.0\n",
      "125  CES3000000008         0.0\n",
      "126    DTCOLNVHFNM         0.0\n",
      "127       DTCTHFNM         0.0\n",
      "128         INVEST         0.0\n",
      "\n",
      "[129 rows x 2 columns]\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de duplications : 13\n",
      "Best Hyperparameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 1000}\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       135\n",
      "           1       0.12      1.00      0.22        19\n",
      "\n",
      "    accuracy                           0.12       154\n",
      "   macro avg       0.06      0.50      0.11       154\n",
      "weighted avg       0.02      0.12      0.03       154\n",
      "\n",
      "\n",
      "Accuracy : 0.12337662337662338\n",
      "\n",
      "Feature Importance :\n",
      "           Feature  Importance\n",
      "0                0         0.0\n",
      "1                1         0.0\n",
      "2                2         0.0\n",
      "3                3         0.0\n",
      "4                4         0.0\n",
      "..             ...         ...\n",
      "124  CES2000000008         0.0\n",
      "125  CES3000000008         0.0\n",
      "126    DTCOLNVHFNM         0.0\n",
      "127       DTCTHFNM         0.0\n",
      "128         INVEST         0.0\n",
      "\n",
      "[129 rows x 2 columns]\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de duplications : 16\n",
      "Best Hyperparameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 1000}\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       135\n",
      "           1       0.12      1.00      0.22        19\n",
      "\n",
      "    accuracy                           0.12       154\n",
      "   macro avg       0.06      0.50      0.11       154\n",
      "weighted avg       0.02      0.12      0.03       154\n",
      "\n",
      "\n",
      "Accuracy : 0.12337662337662338\n",
      "\n",
      "Feature Importance :\n",
      "           Feature  Importance\n",
      "0                0         0.0\n",
      "1                1         0.0\n",
      "2                2         0.0\n",
      "3                3         0.0\n",
      "4                4         0.0\n",
      "..             ...         ...\n",
      "124  CES2000000008         0.0\n",
      "125  CES3000000008         0.0\n",
      "126    DTCOLNVHFNM         0.0\n",
      "127       DTCTHFNM         0.0\n",
      "128         INVEST         0.0\n",
      "\n",
      "[129 rows x 2 columns]\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Nombre de duplications : 19\n",
      "Best Hyperparameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 1000}\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       135\n",
      "           1       0.12      1.00      0.22        19\n",
      "\n",
      "    accuracy                           0.12       154\n",
      "   macro avg       0.06      0.50      0.11       154\n",
      "weighted avg       0.02      0.12      0.03       154\n",
      "\n",
      "\n",
      "Accuracy : 0.12337662337662338\n",
      "\n",
      "Feature Importance :\n",
      "           Feature  Importance\n",
      "0                0         0.0\n",
      "1                1         0.0\n",
      "2                2         0.0\n",
      "3                3         0.0\n",
      "4                4         0.0\n",
      "..             ...         ...\n",
      "124  CES2000000008         0.0\n",
      "125  CES3000000008         0.0\n",
      "126    DTCOLNVHFNM         0.0\n",
      "127       DTCTHFNM         0.0\n",
      "128         INVEST         0.0\n",
      "\n",
      "[129 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "#on combine les sparses factors à toutes les covariates (fred_md_short)\n",
    "factors = pd.read_csv('/home/onyxia/work/Macroeconometrics/data/estimated_factor.csv', index_col='sasdate') \n",
    "factors.index = pd.to_datetime(factors.index)\n",
    "\n",
    "data = pd.merge(factors, fred_md_short, left_index=True, right_index=True, how='inner')\n",
    "data = data.shift(1)\n",
    "\n",
    "data = pd.concat([data, us_rec], axis=1, join='inner').dropna()\n",
    "\n",
    "for k in range(1, 20, 3):\n",
    "\n",
    "    X = data.drop(columns=['USREC'])  # Variables explicatives\n",
    "    y = data['USREC']                  # Variable cible\n",
    "\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Séparer les données en ensemble d'entraînement et de test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=667, stratify=y)\n",
    "\n",
    "    # Convertir X_train en DataFrame\n",
    "    X_train = pd.DataFrame(X_train, columns=X.columns)\n",
    "\n",
    "    # Combiner X_train et y_train pour appliquer la duplication\n",
    "    train_data = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "    # Dupliquer les lignes avec USREC = 1\n",
    "    train_data_with_duplicates = duplicate_usrec(train_data, k)\n",
    "\n",
    "    # Séparer à nouveau les variables explicatives et la cible\n",
    "    X_train_final = train_data_with_duplicates.drop(columns=['USREC'])\n",
    "    y_train_final = train_data_with_duplicates['USREC']\n",
    "\n",
    "    # Instantiate Random Forest model\n",
    "    rf = RandomForestClassifier(random_state=667)\n",
    "    \n",
    "    # Hyperparameter tuning using GridSearchCV\n",
    "    param_grid = {\n",
    "        'n_estimators': [1000],  # Nombre d'arbres dans la forêt\n",
    "        'max_depth': [None, 10, 30],   # Profondeur maximale des arbres\n",
    "        'min_samples_split': [2, 5, 10],   # Nombre minimal d'échantillons pour diviser un noeud\n",
    "        'min_samples_leaf': [1, 4],     # Nombre minimal d'échantillons par feuille\n",
    "        }\n",
    "    \n",
    "    grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "    grid_search.fit(X_train_final, y_train_final)\n",
    "    \n",
    "    #Best model\n",
    "    best_rf = grid_search.best_estimator_\n",
    "\n",
    "    #Predictions on the test set\n",
    "    y_pred = best_rf.predict(X_test)\n",
    "\n",
    "    #Evaluation\n",
    "    classification_rep = classification_report(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    #Feature importance\n",
    "    importances = best_rf.feature_importances_\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'Feature': X.columns if isinstance(X, pd.DataFrame) else [f'PC{i+1}' for i in range(X.shape[1])],\n",
    "        'Importance': importances\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "    # Return results\n",
    "    results = {\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'classification_report': classification_rep,\n",
    "        'accuracy': accuracy,\n",
    "        'feature_importance': feature_importance_df\n",
    "    }\n",
    "\n",
    "    # Display the results\n",
    "    print(\"Nombre de duplications :\", k)\n",
    "    print(\"Best Hyperparameters:\", results['best_params'])\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(results['classification_report'])\n",
    "    print(\"\\nAccuracy :\", results['accuracy'])\n",
    "    print(\"\\nFeature Importance :\")\n",
    "    print(results['feature_importance'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
